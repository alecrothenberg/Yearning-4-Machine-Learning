{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the Feeling: Identifying the Key Emotions of a song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lyrics Preprocessing: To lower, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "# create new folder to hold the preprocessed data\n",
    "# create new text files with the preprocessed data \n",
    "# create three functions to do upper to lower, \n",
    "\n",
    "rawTextDataDir = \"dataset/Lyrics/\"\n",
    "preprocessedDataDir = \"preprocessedData/Lyrics/\"\n",
    "\n",
    "#iterate over all files in rawText and create an associated proccessed file in preprocessed dir\n",
    "\n",
    "# to lower\n",
    "for fileName in os.listdir(rawTextDataDir):\n",
    "    \n",
    "    with open(rawTextDataDir + fileName, 'r') as data:\n",
    "        newFileName = \"processed\" + fileName\n",
    "        \n",
    "        with open (preprocessedDataDir + newFileName, 'a') as newData:\n",
    "            for line in data:\n",
    "                newData.write(line.lower())\n",
    "            newData.close();\n",
    "    data.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punctuatuon removal \n",
    "\n",
    "for fileName in os.listdir(preprocessedDataDir):\n",
    "    with open(preprocessedDataDir + fileName, 'r') as file:\n",
    "        data = file.read()\n",
    "    data = data.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "    with open(preprocessedDataDir + fileName, 'w') as file:\n",
    "        file.write(data)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/markpolkhovskiy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/markpolkhovskiy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#stop word removal\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for fileName in os.listdir(preprocessedDataDir):\n",
    "    with open(preprocessedDataDir + fileName) as file:\n",
    "        data = file.read()\n",
    "    tokens = nltk.word_tokenize(data)\n",
    "    filtered_tokens = [word for word in tokens if not word.lower() in stop_words]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    with open(preprocessedDataDir + fileName, 'w') as file:\n",
    "            file.write(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/markpolkhovskiy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/markpolkhovskiy/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "for fileName in os.listdir(preprocessedDataDir):\n",
    "    with open(preprocessedDataDir + fileName) as file:\n",
    "        data = file.read()\n",
    "    tokens = nltk.word_tokenize(data)\n",
    "    for (index,word) in enumerate(tokens):\n",
    "        tokens[index] = lemmatizer.lemmatize(word)\n",
    "        #tokens[index] = stemmer.stem(word)\n",
    "    lemmatized_tokens = ' '.join(tokens)\n",
    "    with open(preprocessedDataDir + fileName, 'w') as file:\n",
    "        file.write(lemmatized_tokens);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to exatract features and combine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractLoudness(signal):\n",
    "    df_loudness = pd.DataFrame()\n",
    "    S, phase = librosa.magphase(librosa.stft(signal))\n",
    "    rms = librosa.feature.rms(S=S)\n",
    "    df_loudness['Loudness'] = rms[0]\n",
    "    return df_loudness\n",
    "\n",
    "def extractMFCCS(signal, sample_rate):\n",
    "    df_mfccs = pd.DataFrame()\n",
    "    mfccs = librosa.feature.mfcc(y=signal, sr=sample_rate, n_mfcc=12)\n",
    "    for n_mfcc in range(len(mfccs)):\n",
    "        df_mfccs['MFCC_%d'%(n_mfcc+1)] = mfccs.T[n_mfcc]\n",
    "    return df_mfccs\n",
    "\n",
    "def extractZeroCrossingRate(signal):\n",
    "    df_zero_crossing_rate = pd.DataFrame()\n",
    "    zcr = librosa.feature.zero_crossing_rate(y=signal)\n",
    "    df_zero_crossing_rate['ZCR'] = zcr[0]\n",
    "    return df_zero_crossing_rate\n",
    "\n",
    "def extractChroma(signal, sample_rate):\n",
    "    df_chroma = pd.DataFrame()\n",
    "    chromagram = librosa.feature.chroma_stft(y=signal, sr=sample_rate)\n",
    "    for n_chroma in range(len(chromagram)):\n",
    "        df_chroma['Chroma_%d'%(n_chroma+1)] = chromagram.T[n_chroma]\n",
    "    return df_chroma\n",
    "\n",
    "def extractMelSpectrogram(signal, sample_rate):\n",
    "    df_mel_spectrogram = pd.DataFrame()\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=signal, sr=sample_rate, n_mels=12)\n",
    "    for n_mel in range(len(mel_spectrogram)):\n",
    "        df_mel_spectrogram['Mel_Spectrogram_%d'%(n_mel+1)] = mel_spectrogram.T[n_mel]\n",
    "    return df_mel_spectrogram\n",
    "\n",
    "def matrixToVector(matrixFeatures):\n",
    "    vector = []\n",
    "    for label in matrixFeatures.columns:\n",
    "        if label == 'Loudness' or label == 'ZCR':\n",
    "            vector.append(np.mean(matrixFeatures[label].dropna().to_numpy()))\n",
    "        else:\n",
    "            vector.extend(matrixFeatures[label].dropna().to_numpy())\n",
    "    return vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features and flatten it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(audioDir):\n\u001b[0;32m      5\u001b[0m     signal, rate \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39mload(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00maudioDir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m     matrix \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([\n\u001b[1;32m----> 7\u001b[0m         extractLoudness(signal), \n\u001b[0;32m      8\u001b[0m         extractMFCCS(signal, rate), \n\u001b[0;32m      9\u001b[0m         extractZeroCrossingRate(signal),\n\u001b[0;32m     10\u001b[0m         extractChroma(signal, rate),\n\u001b[0;32m     11\u001b[0m         extractMelSpectrogram(signal, rate)])\n\u001b[0;32m     12\u001b[0m     features_vectors\u001b[39m.\u001b[39mappend(matrixToVector(matrix))\n",
      "Cell \u001b[1;32mIn[71], line 3\u001b[0m, in \u001b[0;36mextractLoudness\u001b[1;34m(signal)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextractLoudness\u001b[39m(signal):\n\u001b[0;32m      2\u001b[0m     df_loudness \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\n\u001b[1;32m----> 3\u001b[0m     S, phase \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39;49mmagphase(librosa\u001b[39m.\u001b[39;49mstft(signal))\n\u001b[0;32m      4\u001b[0m     rms \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39mfeature\u001b[39m.\u001b[39mrms(S\u001b[39m=\u001b[39mS)\n\u001b[0;32m      5\u001b[0m     df_loudness[\u001b[39m'\u001b[39m\u001b[39mLoudness\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m rms[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32md:\\anaconda\\envs\\p3-librosa\\lib\\site-packages\\librosa\\core\\spectrum.py:1342\u001b[0m, in \u001b[0;36mmagphase\u001b[1;34m(D, power)\u001b[0m\n\u001b[0;32m   1288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmagphase\u001b[39m(D: np\u001b[39m.\u001b[39mndarray, \u001b[39m*\u001b[39m, power: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mndarray]:\n\u001b[0;32m   1289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Separate a complex-valued spectrogram D into its magnitude (S)\u001b[39;00m\n\u001b[0;32m   1290\u001b[0m \u001b[39m    and phase (P) components, so that ``D = S * P``.\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1339\u001b[0m \n\u001b[0;32m   1340\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1342\u001b[0m     mag \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mabs(D)\n\u001b[0;32m   1344\u001b[0m     \u001b[39m# Prevent NaNs and return magnitude 0, phase 1+0j for zero\u001b[39;00m\n\u001b[0;32m   1345\u001b[0m     zeros_to_ones \u001b[39m=\u001b[39m mag \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "audioDir = 'dataset/Audio'\n",
    "features_vectors = []\n",
    "\n",
    "for file in os.listdir(audioDir):\n",
    "    signal, rate = librosa.load(f'{audioDir}/{file}')\n",
    "    matrix = pd.concat([\n",
    "        extractLoudness(signal), \n",
    "        extractMFCCS(signal, rate), \n",
    "        extractZeroCrossingRate(signal),\n",
    "        extractChroma(signal, rate),\n",
    "        extractMelSpectrogram(signal, rate)])\n",
    "    features_vectors.append(matrixToVector(matrix))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export data into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_vectors = np.asarray(features_vectors)\n",
    "np.savetxt('audio_features.csv', features_vectors, delimiter=',')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load audio feature data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "reader = csv.reader(open(\"audio_features.csv\", \"r\"), delimiter=\",\")\n",
    "x = list(reader)\n",
    "features = np.array(x).astype(\"float\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Processing + Model Building + Model Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, roc_curve, auc, classification_report, accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Audio Features to DataFrame from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "audioFeatures = pd.read_csv('audio_features.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = []\n",
    "with open('./dataset/clusters.txt') as file:\n",
    "    for line in file:\n",
    "        clusters.append(line.split()[1])\n",
    "combined = pd.DataFrame({\"clusters\":clusters, 'text':np.nan})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"./preprocessedData/Lyrics/\"\n",
    "text = pd.DataFrame()\n",
    "combined['text'] = combined['text'].astype('string')\n",
    "for i in os.listdir(path):\n",
    "    textfile = open(path+i)\n",
    "    combined.at[int(i[9:12])-1,'text'] = \"\".join(textfile.readlines())#.split(\" \")\n",
    "    textfile.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine and fill na cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_final = pd.concat([combined, audioFeatures.set_axis(combined.index)], axis=1)\n",
    "combined_final = combined_final.fillna(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the audio data by column (feature) and average by row (file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "for i in range (0, 433):\n",
    "    combined_final['a'+str(i)] = (scaler.fit_transform(combined_final['a'+str(i)].to_numpy().reshape(-1,1)))\n",
    "\n",
    "cols = combined_final.columns.drop(['clusters','text'])\n",
    "combined_final['a0'] = combined_final.loc[:, cols].mean(axis=1)\n",
    "\n",
    "combined_final = combined_final.loc[:, [\"clusters\",\"text\", \"a0\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "scale = StandardScaler() #from sklearn\n",
    "\n",
    "x = combined_final.drop('clusters', axis=1)\n",
    "y = combined_final.loc[:, ['clusters']]\n",
    "\n",
    "#split dataset into training dataset and test dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=8, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting TF*IDF from text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "#train\n",
    "tfidf_vector = tfidf.fit_transform(x_train.loc[:,'text'])\n",
    "tfidf_array = tfidf_vector.toarray()\n",
    "\n",
    "#test\n",
    "tfidf_vector_test = tfidf.transform(x_test.loc[:,'text'])\n",
    "tfidf_array_test = tfidf_vector_test.toarray()\n",
    "\n",
    "#to dataframes\n",
    "tfdf = pd.DataFrame(tfidf_array)\n",
    "tfdf_test = pd.DataFrame(tfidf_array_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting word2Vec from text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "numpy_train = x_train.loc[:,'text'].to_numpy()\n",
    "numpy_test= x_test.loc[:,'text'].to_numpy()\n",
    "\n",
    "trained_w2v = [sentence.split() for sentence in numpy_train]\n",
    "w2v = gensim.models.Word2Vec(trained_w2v, min_count = 40, vector_size = 100, window = 5, sg=0, workers=4)\n",
    "\n",
    "#vectorize the data\n",
    "def vectorize(sentence):\n",
    "    words = sentence.split()\n",
    "    words_vecs = [w2v.wv[word] for word in words if word in w2v.wv]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(100)\n",
    "    words_vecs = np.array(words_vecs)\n",
    "    return words_vecs.mean(axis=0)\n",
    "    \n",
    "x_train_w2v_vect = pd.DataFrame(np.array([vectorize(sentence) for sentence in numpy_train]))\n",
    "x_test_w2v_vect = pd.DataFrame(np.array([vectorize(sentence) for sentence in numpy_test]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concat all features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.concat([x_train, tfdf.set_axis(x_train.index), x_train_w2v_vect.set_axis(x_train.index)], axis=1)\n",
    "x_train = x_train.drop('text', axis=1)\n",
    "#x_train = x_train.rename({'0': 'audio'}, axis=1)\n",
    "x_train.columns = x_train.columns.astype(str)\n",
    "\n",
    "x_test = pd.concat([x_test, tfdf_test.set_axis(x_test.index), x_test_w2v_vect.set_axis(x_test.index)], axis=1)\n",
    "x_test = x_test.drop('text', axis=1)\n",
    "#x_test = x_test.rename({'0': 'audio'}, axis=1)\n",
    "x_test.columns = x_test.columns.astype(str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\p3-librosa\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "d:\\anaconda\\envs\\p3-librosa\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression Accuracy: 0.37\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.19      0.15      0.17        48\n",
      "           2       0.42      0.22      0.29        51\n",
      "           3       0.41      0.75      0.53        55\n",
      "           4       0.45      0.30      0.36        63\n",
      "           5       0.34      0.43      0.38        54\n",
      "\n",
      "    accuracy                           0.37       271\n",
      "   macro avg       0.36      0.37      0.34       271\n",
      "weighted avg       0.37      0.37      0.35       271\n",
      "\n",
      "\n",
      "Decision Tree Accuracy: 0.28\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.19      0.21      0.20        48\n",
      "           2       0.21      0.16      0.18        51\n",
      "           3       0.36      0.51      0.42        55\n",
      "           4       0.32      0.29      0.30        63\n",
      "           5       0.27      0.22      0.24        54\n",
      "\n",
      "    accuracy                           0.28       271\n",
      "   macro avg       0.27      0.28      0.27       271\n",
      "weighted avg       0.27      0.28      0.27       271\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\p3-librosa\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:215: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 0.31\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.21      0.29      0.25        48\n",
      "           2       0.31      0.29      0.30        51\n",
      "           3       0.32      0.44      0.37        55\n",
      "           4       0.38      0.32      0.35        63\n",
      "           5       0.37      0.20      0.26        54\n",
      "\n",
      "    accuracy                           0.31       271\n",
      "   macro avg       0.32      0.31      0.31       271\n",
      "weighted avg       0.32      0.31      0.31       271\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\p3-librosa\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.28\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.18      0.19      0.18        48\n",
      "           2       0.18      0.25      0.21        51\n",
      "           3       0.40      0.35      0.37        55\n",
      "           4       0.34      0.41      0.37        63\n",
      "           5       0.35      0.17      0.23        54\n",
      "\n",
      "    accuracy                           0.28       271\n",
      "   macro avg       0.29      0.27      0.27       271\n",
      "weighted avg       0.30      0.28      0.28       271\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avamc_srsebwe\\AppData\\Local\\Temp\\ipykernel_16844\\3856402857.py:12: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model.fit(x_train, y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.35\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.33      0.31      0.32        48\n",
      "           2       0.30      0.14      0.19        51\n",
      "           3       0.34      0.78      0.48        55\n",
      "           4       0.33      0.27      0.30        63\n",
      "           5       0.44      0.22      0.30        54\n",
      "\n",
      "    accuracy                           0.35       271\n",
      "   macro avg       0.35      0.34      0.32       271\n",
      "weighted avg       0.35      0.35      0.32       271\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\p3-librosa\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Classifier Accuracy: 0.35\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.20      0.19      0.19        48\n",
      "           2       0.37      0.20      0.26        51\n",
      "           3       0.38      0.60      0.46        55\n",
      "           4       0.41      0.27      0.33        63\n",
      "           5       0.36      0.46      0.40        54\n",
      "\n",
      "    accuracy                           0.35       271\n",
      "   macro avg       0.34      0.34      0.33       271\n",
      "weighted avg       0.35      0.35      0.33       271\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# modelSelector holds all of the different models we will use to train the data\n",
    "modelSelector = {\n",
    "    'logistic regression' : LogisticRegression(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Support Vector Classifier': SVC(kernel='linear')\n",
    "}\n",
    "\n",
    "for modelName, model in modelSelector.items():\n",
    "    model.fit(x_train, y_train) \n",
    "    y_pred = model.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{modelName} Accuracy: {accuracy:.2f}\")  # helps with analysis, we may wanna move this to further modulize implementation\n",
    "    print(classification_report(y_test, y_pred, target_names=model.classes_))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b6ddfb2dcac28d85c50c41a252d540a7fa8764b66ffa4029a84ab1fab305572"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
